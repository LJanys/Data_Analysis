[
["logistic-regression.html", "Chapter 17 Logistic Regression", " Chapter 17 Logistic Regression Consider the following model \\[\\begin{equation}P(Y_i=y_i)=\\pi_i^{y_i}(1-\\pi_i)^{1-y_i}\\end{equation}\\] where \\[\\begin{equation} \\pi_i=\\frac{exp\\{ \\mathbf{x}_i&#39;\\boldsymbol{\\beta} \\}}{1+exp\\{ \\mathbf{\\mathbf{x}}_i&#39;\\boldsymbol{\\beta} \\}} \\end{equation}\\] Simulate this model with the probabilities as described above with the following values: \\(n =1000\\) \\(\\beta_0=-2, \\, \\beta_1=0.1,\\, \\beta2=1\\). \\(x_{0i}=1 \\, \\forall \\,i\\) , \\(x_{1i}\\sim \\mathcal{U}(18,60), \\, x_{2i}\\sim \\mathcal{B}(0.5)\\). Estimate \\(\\beta_0, \\beta_1, \\beta_2\\) via maximum likelihood and calculate the standard errors. Propose and calculate a suitable method for the interpretation of the coefficients as discussed in the lecture. ##The logit transformation# ####Generate a vector of probabilities######## pi=seq(0,0.99999999,le=100) logit&lt;-function(x) { logit&lt;-log(x/(1-x)) return(logit) } plot(logit(pi),pi,type=&quot;l&quot;, xlim=c(-5,5), main=&quot;The Logit Transformation&quot;,xlab=&quot;logit&quot;,ylab=&quot;probability &quot;, cex.main=0.7, cex.lab=0.7,cex.axis=0.7) #Maximum Likelihood estimation: Logit### ##General Syntax### library(miscTools)#the maxLik package acts as a wrapper for the more basic &quot;optim&quot;, the library miscTools is required. library(maxLik) loglike&lt;-function(beta)#Define a function that takes only the parameter vector as arguments. { ll &lt;- &quot;my log likelihood function&quot; #depending on your optimization routine, #check whether you need the negative or the positive log likelihood! return(ll) } estim&lt;-maxBFGS(loglike,finalHessian=TRUE,start=c(.,.))###initialize the optimization, #pass on starting values and store the results in estim estim.par&lt;-estim$estimate ### store the paramter estimates in a variable &quot;estim.par&quot; "]
]
